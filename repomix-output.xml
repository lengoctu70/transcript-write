This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: tests/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
tests/
  fixtures/
    sample.srt
    sample.vtt
  test_chunker.py
  test_cost_estimator.py
  test_integration.py
  test_llm_processor.py
  test_parser.py
  test_validator.py
  test_writer.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="tests/fixtures/sample.vtt">
WEBVTT

00:00:01.000 --> 00:00:05.000
Uh, hello everyone, welcome to the lecture.

00:00:06.000 --> 00:00:10.000
So, like, today we're going to talk about machine learning.

00:00:11.000 --> 00:00:15.000
You know, machine learning is basically a subset of AI.

00:00:16.000 --> 00:00:20.000
The concept is really quite simple, actually.

00:00:21.000 --> 00:00:25.000
Why does this matter? Because it helps automate decisions.
</file>

<file path="tests/test_integration.py">
"""Integration tests for full pipeline"""
import pytest
from pathlib import Path
from unittest.mock import Mock, patch

from src import (
    TranscriptParser,
    SmartChunker,
    LLMProcessor,
    OutputValidator,
    MarkdownWriter,
    CostEstimator
)


class TestFullPipeline:
    """Test complete processing pipeline"""

    @pytest.fixture
    def sample_srt(self, tmp_path):
        """Create sample SRT file"""
        content = """1
00:00:01,000 --> 00:00:05,000
Uh, hello everyone, welcome to the lecture.

2
00:00:06,000 --> 00:00:10,000
So, like, today we're going to talk about machine learning.

3
00:00:11,000 --> 00:00:15,000
You know, machine learning is basically a subset of AI.

4
00:00:16,000 --> 00:00:20,000
The concept is really quite simple, actually.

5
00:00:21,000 --> 00:00:25,000
Why does this matter? Because it helps automate decisions.
"""
        srt_file = tmp_path / "sample.srt"
        srt_file.write_text(content)
        return srt_file

    @pytest.fixture
    def prompt_template(self, tmp_path):
        """Create prompt template"""
        content = """Clean this transcript.

[VIDEO INFO]
Title: {{fileName}}

[TRANSCRIPT TO PROCESS]
{{chunkText}}
"""
        prompt_file = tmp_path / "prompt.txt"
        prompt_file.write_text(content)
        return prompt_file

    def test_parse_chunk_flow(self, sample_srt):
        """Test parsing and chunking"""
        # Parse
        parser = TranscriptParser()
        segments = parser.parse(sample_srt)

        assert len(segments) > 0
        assert all(s.text for s in segments)

        # Convert to text
        text = parser.to_plain_text(segments)
        assert "[00:00:01]" in text

        # Chunk
        chunker = SmartChunker(chunk_size=500, overlap=50)
        chunks = chunker.chunk_transcript(text)

        assert len(chunks) >= 1
        assert chunks[0].text

    @patch('src.llm_processor.anthropic.Anthropic')
    def test_full_pipeline(self, mock_anthropic, sample_srt, prompt_template, tmp_path):
        """Test complete pipeline with mocked API"""
        # Setup mock
        mock_response = Mock()
        mock_response.content = [Mock(text="[00:00:01]\nCleaned content here.")]
        mock_response.usage.input_tokens = 100
        mock_response.usage.output_tokens = 50
        mock_anthropic.return_value.messages.create.return_value = mock_response

        # Parse
        parser = TranscriptParser()
        segments = parser.parse(sample_srt)
        text = parser.to_plain_text(segments)

        # Chunk
        chunker = SmartChunker(chunk_size=500, overlap=50)
        chunks = chunker.chunk_transcript(text)

        # Estimate cost
        estimator = CostEstimator()
        template = prompt_template.read_text()
        estimate = estimator.estimate_total(chunks, template)

        assert estimate.total_cost > 0
        assert estimate.chunks == len(chunks)

        # Process (mocked)
        processor = LLMProcessor(api_key="test-key")
        results = processor.process_all_chunks(chunks, template, "Test Video")

        assert len(results) == len(chunks)

        # Validate
        validator = OutputValidator()
        validation = validator.validate_all(results)

        # Should have no context markers in output
        context_errors = [i for i in validation.issues
                        if i.rule == "context_marker_in_output"]
        assert len(context_errors) == 0

        # Write
        writer = MarkdownWriter(output_dir=str(tmp_path / "output"))
        summary = {
            "chunks_processed": len(results),
            "total_input_tokens": 100,
            "total_output_tokens": 50,
            "total_cost": 0.01,
            "model": "test-model"
        }
        md_path, json_path = writer.write(results, "Test Video", summary)

        assert md_path.exists()
        assert json_path.exists()
        assert "Test Video" in md_path.read_text()


class TestErrorHandling:
    """Test error handling scenarios"""

    def test_invalid_file_format(self, tmp_path):
        """Reject unsupported file format"""
        txt_file = tmp_path / "invalid.txt"
        txt_file.write_text("Not a subtitle file")

        parser = TranscriptParser()
        with pytest.raises(ValueError, match="Unsupported format"):
            parser.parse(txt_file)

    def test_empty_file(self, tmp_path):
        """Handle empty file gracefully"""
        empty_srt = tmp_path / "empty.srt"
        empty_srt.write_text("")

        parser = TranscriptParser()
        segments = parser.parse(empty_srt)

        assert segments == []

    def test_malformed_srt(self, tmp_path):
        """Handle malformed SRT gracefully"""
        bad_srt = tmp_path / "bad.srt"
        bad_srt.write_text("Not valid SRT content\nJust random text")

        parser = TranscriptParser()
        # Should not crash, may return empty or partial
        try:
            segments = parser.parse(bad_srt)
            # If it parses, should be empty or have valid segments
            assert isinstance(segments, list)
        except Exception:
            # Some errors are acceptable for malformed input
            pass
</file>

<file path="tests/fixtures/sample.srt">
1
00:00:01,000 --> 00:00:04,000
Hello everyone, welcome to today's lecture.

2
00:00:05,000 --> 00:00:08,000
Uh, so, like, today we're going to talk about machine learning.

3
00:00:09,000 --> 00:00:12,000
You know, machine learning is basically a subset of AI.
</file>

<file path="tests/test_chunker.py">
"""Tests for smart chunker"""
import pytest
from src.chunker import SmartChunker, Chunk


class TestSmartChunker:

    def test_basic_chunking(self):
        """Chunk text at target size"""
        text = "A" * 2500
        chunker = SmartChunker(chunk_size=1000, overlap=100)
        chunks = chunker.chunk_transcript(text)

        assert len(chunks) >= 2
        assert chunks[0].index == 0

    def test_sentence_boundary_split(self):
        """Prefer splitting at sentence boundaries"""
        text = "First sentence here. Second sentence follows. " * 50
        chunker = SmartChunker(chunk_size=100, overlap=20)
        chunks = chunker.chunk_transcript(text)

        for chunk in chunks[:-1]:
            assert chunk.text.rstrip().endswith(".")

    def test_context_buffer(self):
        """Second chunk has context from first"""
        text = "First part. " * 100 + "Second part. " * 100
        chunker = SmartChunker(chunk_size=500, overlap=100)
        chunks = chunker.chunk_transcript(text)

        assert chunks[0].context_buffer is None
        assert chunks[1].context_buffer is not None
        assert len(chunks[1].context_buffer) > 0

    def test_timestamp_extraction(self):
        """Extract timestamp from chunk"""
        text = "[00:01:30] This is the content."
        chunker = SmartChunker()
        chunks = chunker.chunk_transcript(text)

        assert chunks[0].start_timestamp == "00:01:30"

    def test_full_text_for_llm(self):
        """Build complete text for LLM with context"""
        chunk = Chunk(
            index=1,
            text="New content here",
            start_timestamp="00:01:00",
            context_buffer="Previous context"
        )

        llm_text = chunk.full_text_for_llm
        assert "[CONTEXT FROM PREVIOUS SECTION]" in llm_text
        assert "Previous context" in llm_text
        assert "[NEW CONTENT TO PROCESS]" in llm_text
        assert "New content here" in llm_text
</file>

<file path="tests/test_cost_estimator.py">
"""Tests for cost estimator"""
import pytest
from unittest.mock import patch, MagicMock
from src.cost_estimator import (
    CostBreakdown,
    CostEstimator
)
from src.chunker import Chunk


class TestCostBreakdown:
    """Test CostBreakdown dataclass"""

    def test_cost_breakdown_creation(self):
        """Create CostBreakdown with all fields"""
        breakdown = CostBreakdown(
            input_tokens=1000,
            output_tokens_est=800,
            input_cost=0.003,
            output_cost=0.012,
            total_cost=0.015,
            chunks=2,
            processing_time_minutes=10.5
        )
        assert breakdown.input_tokens == 1000
        assert breakdown.output_tokens_est == 800
        assert breakdown.input_cost == 0.003
        assert breakdown.output_cost == 0.012
        assert breakdown.total_cost == 0.015
        assert breakdown.chunks == 2
        assert breakdown.processing_time_minutes == 10.5


class TestCostEstimator:
    """Test CostEstimator class"""

    def test_init_default_model(self):
        """Initialize with default model"""
        estimator = CostEstimator()
        assert estimator.model == "claude-3-5-sonnet-20241022"

    def test_init_custom_model(self):
        """Initialize with custom model"""
        estimator = CostEstimator(model="claude-3-5-haiku-20241022")
        assert estimator.model == "claude-3-5-haiku-20241022"

    def test_count_tokens_with_tiktoken(self):
        """Count tokens using tiktoken when available"""
        with patch('src.cost_estimator.HAS_TIKTOKEN', True):
            with patch('src.cost_estimator.tiktoken') as mock_tiktoken:
                mock_encoder = MagicMock()
                mock_encoder.encode.return_value = [1, 2, 3, 4, 5]
                mock_tiktoken.get_encoding.return_value = mock_encoder

                estimator = CostEstimator()
                count = estimator.count_tokens("Test text here")

                assert count == 5
                mock_encoder.encode.assert_called_once_with("Test text here")

    def test_count_tokens_fallback_to_char_division(self):
        """Fallback to character / 4 when tiktoken unavailable"""
        with patch('src.cost_estimator.HAS_TIKTOKEN', False):
            estimator = CostEstimator()
            count = estimator.count_tokens("ABCD")  # 4 chars
            assert count == 1  # 4 / 4 = 1

    def test_count_tokens_tiktoken_error_fallback(self):
        """Fallback when tiktoken raises exception"""
        with patch('src.cost_estimator.HAS_TIKTOKEN', True):
            with patch('src.cost_estimator.tiktoken') as mock_tiktoken:
                mock_encoder = MagicMock()
                mock_encoder.encode.side_effect = Exception("Encoding failed")
                mock_tiktoken.get_encoding.return_value = mock_encoder

                estimator = CostEstimator()
                count = estimator.count_tokens("ABCD")  # 4 chars
                assert count == 1  # Fallback to 4/4

    def test_estimate_chunk_tokens(self):
        """Estimate tokens for single chunk"""
        estimator = CostEstimator()
        with patch.object(estimator, 'count_tokens', return_value=100):
            input_tokens, output_tokens = estimator.estimate_chunk_tokens(
                chunk_text="Test chunk content",
                prompt_template="Test prompt template"
            )

            assert input_tokens == 200  # 100 (chunk) + 100 (template)
            assert output_tokens == 80  # 100 * 0.8

    def test_estimate_total_single_chunk(self):
        """Estimate total cost for single chunk"""
        estimator = CostEstimator()
        chunk = Chunk(
            index=0,
            text="Test content that should be about this long",
            start_timestamp="00:00:00"
        )

        # Mock count_tokens to return different values for template vs content
        def mock_count_tokens(text):
            if "Prompt template" in text:
                return 100  # Template tokens
            elif "Test content" in text:
                return 500  # Chunk text tokens
            elif "[CONTEXT" in text:
                return 200  # Context buffer tokens
            return 300

        with patch.object(estimator, 'count_tokens', side_effect=mock_count_tokens):
            breakdown = estimator.estimate_total(
                chunks=[chunk],
                prompt_template="Prompt template"
            )

            assert breakdown.chunks == 1
            # Input: template (100) + chunk.full_text_for_llm (which contains text + maybe context)
            # The exact value depends on how full_text_for_llm is constructed
            assert breakdown.input_tokens > 0
            assert breakdown.output_tokens_est > 0
            assert breakdown.total_cost > 0
            # Time: (1 chunk * 5 sec) / 60 = 0.0833... rounds to 0.1
            assert breakdown.processing_time_minutes == 0.1

    def test_estimate_total_multiple_chunks(self):
        """Estimate total cost for multiple chunks"""
        estimator = CostEstimator()
        chunks = [
            Chunk(index=0, text="Content one", start_timestamp="00:00:00"),
            Chunk(index=1, text="Content two", start_timestamp="00:01:00"),
            Chunk(index=2, text="Content three", start_timestamp="00:02:00")
        ]

        # Mock count_tokens to return different values
        call_count = [0]
        def mock_count_tokens(text):
            call_count[0] += 1
            if "Template" in text:
                return 100  # Template tokens
            return 300  # Content tokens

        with patch.object(estimator, 'count_tokens', side_effect=mock_count_tokens):
            breakdown = estimator.estimate_total(
                chunks=chunks,
                prompt_template="Template"
            )

            assert breakdown.chunks == 3
            assert breakdown.input_tokens > 0
            assert breakdown.output_tokens_est > 0
            assert breakdown.total_cost > 0
            # Time: (3 chunks * 5 sec) / 60 = 0.25 rounds to 0.2 or 0.3
            assert breakdown.processing_time_minutes in [0.2, 0.3]

    def test_estimate_total_haiku_model(self):
        """Estimate costs for Haiku model"""
        estimator = CostEstimator(model="claude-3-5-haiku-20241022")
        chunk = Chunk(
            index=0,
            text="Test content",
            start_timestamp="00:00:00"
        )

        def mock_count_tokens(text):
            if "Template" in text:
                return 100
            return 1000

        with patch.object(estimator, 'count_tokens', side_effect=mock_count_tokens):
            breakdown = estimator.estimate_total(
                chunks=[chunk],
                prompt_template="Template"
            )

            # Haiku pricing: $0.001/1K input, $0.005/1K output
            assert breakdown.input_cost >= 0
            assert breakdown.output_cost >= 0
            assert breakdown.total_cost >= 0
            # Time: (1 chunk * 3 sec) / 60 = 0.05 rounds to 0.1
            assert breakdown.processing_time_minutes == 0.1

            # Verify Haiku pricing is used (cheaper than Sonnet)
            # Just check that some cost was calculated
            assert breakdown.total_cost > 0

    def test_estimate_total_unknown_model(self):
        """Fallback to Sonnet pricing for unknown models"""
        estimator = CostEstimator(model="unknown-model")
        chunk = Chunk(
            index=0,
            text="Test",
            start_timestamp="00:00:00"
        )

        with patch.object(estimator, 'count_tokens', return_value=100):
            breakdown = estimator.estimate_total(
                chunks=[chunk],
                prompt_template="Template"
            )

            # Should use Sonnet pricing as fallback
            assert breakdown.input_cost > 0

    def test_format_estimate(self):
        """Format estimate as readable string"""
        breakdown = CostBreakdown(
            input_tokens=1500,
            output_tokens_est=1200,
            input_cost=0.0045,
            output_cost=0.018,
            total_cost=0.0225,
            chunks=3,
            processing_time_minutes=15.0
        )

        estimator = CostEstimator()
        formatted = estimator.format_estimate(breakdown)

        assert "**Cost Estimate**" in formatted
        assert "1,500" in formatted
        assert "0.0045" in formatted
        assert "~1,200" in formatted
        assert "0.0225" in formatted
        assert "3" in formatted
        assert "15.0" in formatted

    def test_format_estimate_with_large_numbers(self):
        """Format estimate with large token counts"""
        breakdown = CostBreakdown(
            input_tokens=150000,
            output_tokens_est=120000,
            input_cost=0.45,
            output_cost=1.80,
            total_cost=2.25,
            chunks=100,
            processing_time_minutes=500.0
        )

        estimator = CostEstimator()
        formatted = estimator.format_estimate(breakdown)

        assert "150,000" in formatted
        assert "120,000" in formatted

    def test_estimate_total_with_context_buffer(self):
        """Estimate chunks that include context buffer"""
        estimator = CostEstimator()
        chunk = Chunk(
            index=1,
            text="New content",
            start_timestamp="00:01:00",
            context_buffer="Previous context for continuity"
        )

        with patch.object(estimator, 'count_tokens', return_value=100):
            breakdown = estimator.estimate_total(
                chunks=[chunk],
                prompt_template="Template"
            )

            # Should account for full_text_for_llm (includes context)
            assert breakdown.input_tokens > 0

    def test_estimate_total_empty_chunks(self):
        """Handle empty chunk list"""
        estimator = CostEstimator()
        with patch.object(estimator, 'count_tokens', return_value=0):
            breakdown = estimator.estimate_total(
                chunks=[],
                prompt_template="Template"
            )

            assert breakdown.chunks == 0
            assert breakdown.total_cost == 0

    def test_encoder_lazy_loading(self):
        """Encoder is loaded only when needed"""
        with patch('src.cost_estimator.HAS_TIKTOKEN', True):
            with patch('src.cost_estimator.tiktoken') as mock_tiktoken:
                mock_encoder = MagicMock()
                mock_tiktoken.get_encoding.return_value = mock_encoder

                estimator = CostEstimator()
                assert estimator._encoder is None

                # Access encoder property
                _ = estimator.encoder
                assert estimator._encoder is not None
                mock_tiktoken.get_encoding.assert_called_once_with("cl100k_base")

    def test_encoder_without_tiktoken(self):
        """Return None when tiktoken not available"""
        with patch('src.cost_estimator.HAS_TIKTOKEN', False):
            estimator = CostEstimator()
            assert estimator.encoder is None

    def test_pricing_dict_completeness(self):
        """Verify pricing dict has required models"""
        estimator = CostEstimator()
        assert "claude-3-5-sonnet-20241022" in estimator.PRICING
        assert "claude-3-5-haiku-20241022" in estimator.PRICING

        for model, prices in estimator.PRICING.items():
            assert "input" in prices
            assert "output" in prices
            assert isinstance(prices["input"], (int, float))
            assert isinstance(prices["output"], (int, float))

    def test_time_per_chunk_dict(self):
        """Verify time per chunk dict"""
        estimator = CostEstimator()
        assert "claude-3-5-sonnet-20241022" in estimator.TIME_PER_CHUNK
        assert "claude-3-5-haiku-20241022" in estimator.TIME_PER_CHUNK

        for model, time in estimator.TIME_PER_CHUNK.items():
            assert isinstance(time, (int, float))
            assert time > 0

    def test_estimate_chunk_tokens_ratio(self):
        """Verify output token estimation ratio is 0.8"""
        estimator = CostEstimator()

        with patch.object(estimator, 'count_tokens', return_value=1000):
            input_t, output_t = estimator.estimate_chunk_tokens(
                chunk_text="Test",
                prompt_template="Template"
            )

            # output_estimate = chunk_tokens * 0.8
            expected_output = 800
            assert output_t == expected_output

    def test_format_estimate_rounding(self):
        """Verify formatted output displays properly"""
        breakdown = CostBreakdown(
            input_tokens=1000,
            output_tokens_est=800,
            input_cost=0.003333,
            output_cost=0.015666,
            total_cost=0.018999,
            chunks=2,
            processing_time_minutes=8.333
        )

        estimator = CostEstimator()
        formatted = estimator.format_estimate(breakdown)

        # Check that formatted output contains expected values
        assert "1,000" in formatted  # input_tokens formatted
        assert "800" in formatted  # output tokens
        assert "2" in formatted  # chunks
        # Costs should be displayed (format may vary)
        assert "0.0033" in formatted or "0.003" in formatted
</file>

<file path="tests/test_llm_processor.py">
"""Tests for LLM processor with mocked API calls"""
import pytest
from unittest.mock import Mock, patch, MagicMock
from src.chunker import Chunk
from src.llm_processor import (
    LLMProcessor,
    ProcessedChunk,
    ProcessingError,
    process_transcript
)


class TestProcessedChunk:
    """Test ProcessedChunk dataclass"""

    def test_processed_chunk_creation(self):
        """Create ProcessedChunk with all fields"""
        chunk = ProcessedChunk(
            chunk_index=0,
            original_text="Original text",
            cleaned_text="Cleaned text",
            input_tokens=100,
            output_tokens=80,
            cost=0.0015,
            model="claude-3-5-sonnet-20241022"
        )
        assert chunk.chunk_index == 0
        assert chunk.original_text == "Original text"
        assert chunk.cleaned_text == "Cleaned text"
        assert chunk.input_tokens == 100
        assert chunk.output_tokens == 80
        assert chunk.cost == 0.0015


class TestProcessingError:
    """Test ProcessingError exception"""

    def test_processing_error_creation(self):
        """Create ProcessingError with recoverable flag"""
        error = ProcessingError(
            chunk_index=1,
            message="API timeout",
            recoverable=True
        )
        assert error.chunk_index == 1
        assert error.message == "API timeout"
        assert error.recoverable is True
        assert "Chunk 1: API timeout" in str(error)


class TestLLMProcessor:
    """Test LLMProcessor class"""

    def test_init_with_api_key(self):
        """Initialize with explicit API key"""
        with patch('src.llm_processor.anthropic.Anthropic'):
            processor = LLMProcessor(api_key="test-key-123")
            assert processor.api_key == "test-key-123"
            assert processor.model == "claude-3-5-sonnet-20241022"

    def test_init_without_api_key_raises_error(self):
        """Raise ValueError when no API key provided"""
        with patch.dict('os.environ', {}, clear=True):
            with pytest.raises(ValueError, match="API key required"):
                LLMProcessor(api_key=None)

    def test_init_from_env_var(self):
        """Initialize API key from environment variable"""
        with patch.dict('os.environ', {'ANTHROPIC_API_KEY': 'env-key-456'}):
            with patch('src.llm_processor.anthropic.Anthropic'):
                processor = LLMProcessor()
                assert processor.api_key == "env-key-456"

    def test_load_prompt_template_default_path(self):
        """Load prompt template from default path"""
        with patch('builtins.open', create=True) as mock_open:
            mock_open.return_value.__enter__.return_value.read.return_value = "Template content"
            with patch('src.llm_processor.anthropic.Anthropic'):
                processor = LLMProcessor(api_key="test-key")
                template = processor.load_prompt_template()
                assert template == "Template content"

    def test_load_prompt_template_missing_file(self):
        """Raise FileNotFoundError when template file missing"""
        with patch('src.llm_processor.anthropic.Anthropic'):
            processor = LLMProcessor(api_key="test-key")
            with pytest.raises(FileNotFoundError, match="Prompt template not found"):
                processor.load_prompt_template("/nonexistent/path.txt")

    def test_cost_calculation_sonnet(self):
        """Calculate cost for Sonnet model"""
        with patch('src.llm_processor.anthropic.Anthropic'):
            processor = LLMProcessor(api_key="test-key", model="claude-3-5-sonnet-20241022")
            cost = processor._calculate_cost(input_tokens=1000, output_tokens=500)
            # Sonnet: $0.003/1K input, $0.015/1K output
            # (1000/1000 * 0.003) + (500/1000 * 0.015) = 0.003 + 0.0075 = 0.0105
            assert cost == 0.0105

    def test_cost_calculation_haiku(self):
        """Calculate cost for Haiku model"""
        with patch('src.llm_processor.anthropic.Anthropic'):
            processor = LLMProcessor(api_key="test-key", model="claude-3-5-haiku-20241022")
            cost = processor._calculate_cost(input_tokens=1000, output_tokens=500)
            # Haiku: $0.001/1K input, $0.005/1K output
            # (1000/1000 * 0.001) + (500/1000 * 0.005) = 0.001 + 0.0025 = 0.0035
            assert cost == 0.0035

    def test_build_prompt(self):
        """Build prompt with chunk and template"""
        chunk = Chunk(
            index=0,
            text="New content",
            start_timestamp="00:01:00",
            context_buffer="Previous content"
        )
        template = "Video: {{fileName}}\nContent: {{chunkText}}"

        with patch('src.llm_processor.anthropic.Anthropic'):
            processor = LLMProcessor(api_key="test-key")
            prompt = processor._build_prompt(chunk, template, "Test Video")

            assert "Video: Test Video" in prompt
            assert "[CONTEXT FROM PREVIOUS SECTION]" in prompt
            assert "Previous content" in prompt
            assert "[NEW CONTENT TO PROCESS]" in prompt
            assert "New content" in prompt

    @patch('src.llm_processor.anthropic.Anthropic')
    def test_process_chunk_success(self, mock_anthropic):
        """Successfully process a chunk"""
        # Setup mock response
        mock_response = Mock()
        mock_response.content = [Mock(text="Cleaned output text")]
        mock_response.usage.input_tokens = 100
        mock_response.usage.output_tokens = 80

        mock_client = Mock()
        mock_client.messages.create.return_value = mock_response
        mock_anthropic.return_value = mock_client

        # Create chunk and process
        chunk = Chunk(index=0, text="Original text", start_timestamp="00:00:00")
        template = "Clean: {{chunkText}}"

        processor = LLMProcessor(api_key="test-key")
        result = processor.process_chunk(chunk, template, "Test Video")

        assert result.chunk_index == 0
        assert result.original_text == "Original text"
        assert result.cleaned_text == "Cleaned output text"
        assert result.input_tokens == 100
        assert result.output_tokens == 80
        assert result.model == "claude-3-5-sonnet-20241022"

    @patch('src.llm_processor.anthropic.Anthropic')
    def test_process_all_chunks_with_callback(self, mock_anthropic):
        """Process multiple chunks with progress callback"""
        # Setup mock response
        mock_response = Mock()
        mock_response.content = [Mock(text="Cleaned")]
        mock_response.usage.input_tokens = 50
        mock_response.usage.output_tokens = 40

        mock_client = Mock()
        mock_client.messages.create.return_value = mock_response
        mock_anthropic.return_value = mock_client

        chunks = [
            Chunk(index=0, text="Text 0", start_timestamp="00:00:00"),
            Chunk(index=1, text="Text 1", start_timestamp="00:01:00"),
            Chunk(index=2, text="Text 2", start_timestamp="00:02:00"),
        ]
        template = "Clean: {{chunkText}}"

        # Track callback calls
        callback_calls = []
        def track_progress(current, total):
            callback_calls.append((current, total))

        processor = LLMProcessor(api_key="test-key")
        results = processor.process_all_chunks(chunks, template, progress_callback=track_progress)

        assert len(results) == 3
        assert callback_calls == [(1, 3), (2, 3), (3, 3)]

    @patch('src.llm_processor.anthropic.Anthropic')
    def test_process_all_chunks_without_callback(self, mock_anthropic):
        """Process chunks without progress callback"""
        mock_response = Mock()
        mock_response.content = [Mock(text="Cleaned")]
        mock_response.usage.input_tokens = 50
        mock_response.usage.output_tokens = 40

        mock_client = Mock()
        mock_client.messages.create.return_value = mock_response
        mock_anthropic.return_value = mock_client

        chunks = [
            Chunk(index=0, text="Text", start_timestamp="00:00:00"),
        ]
        template = "Clean: {{chunkText}}"

        processor = LLMProcessor(api_key="test-key")
        results = processor.process_all_chunks(chunks, template)

        assert len(results) == 1


class TestProcessTranscript:
    """Test convenience function"""

    @patch('src.llm_processor.anthropic.Anthropic')
    def test_process_transcript_returns_summary(self, mock_anthropic):
        """Process transcript and return summary with totals"""
        mock_response = Mock()
        mock_response.content = [Mock(text="Cleaned")]
        mock_response.usage.input_tokens = 100
        mock_response.usage.output_tokens = 80

        mock_client = Mock()
        mock_client.messages.create.return_value = mock_response
        mock_anthropic.return_value = mock_client

        chunks = [
            Chunk(index=0, text="Text 0", start_timestamp="00:00:00"),
            Chunk(index=1, text="Text 1", start_timestamp="00:01:00"),
        ]

        with patch('builtins.open', create=True) as mock_open:
            mock_open.return_value.__enter__.return_value.read.return_value = "Template"
            results, summary = process_transcript(
                chunks=chunks,
                api_key="test-key",
                video_title="Test Video"
            )

        assert len(results) == 2
        assert summary["chunks_processed"] == 2
        assert summary["total_input_tokens"] == 200
        assert summary["total_output_tokens"] == 160
        assert summary["model"] == "claude-3-5-sonnet-20241022"
        assert "total_cost" in summary
</file>

<file path="tests/test_parser.py">
"""Tests for transcript parser"""
import pytest
from src.transcript_parser import TranscriptParser, TranscriptSegment


class TestTranscriptParser:

    def test_parse_srt(self, tmp_path):
        """Parse valid SRT file"""
        srt_content = """1
00:00:01,000 --> 00:00:04,000
Hello, welcome to the lecture.

2
00:00:05,000 --> 00:00:08,000
Today we'll learn about Python.
"""
        srt_file = tmp_path / "test.srt"
        srt_file.write_text(srt_content)

        parser = TranscriptParser()
        segments = parser.parse(srt_file)

        assert len(segments) == 2
        assert segments[0].text == "Hello, welcome to the lecture."
        assert segments[0].start_time == "00:00:01"

    def test_deduplicate_segments(self, tmp_path):
        """Remove consecutive duplicate lines"""
        srt_content = """1
00:00:01,000 --> 00:00:02,000
Hello world

2
00:00:02,000 --> 00:00:03,000
Hello world

3
00:00:03,000 --> 00:00:04,000
Different text
"""
        srt_file = tmp_path / "test.srt"
        srt_file.write_text(srt_content)

        parser = TranscriptParser()
        segments = parser.parse(srt_file)

        assert len(segments) == 2

    def test_to_plain_text(self):
        """Convert segments to plain text"""
        segments = [
            TranscriptSegment(1, "00:00:01", "00:00:04", "First line"),
            TranscriptSegment(2, "00:00:05", "00:00:08", "Second line"),
        ]

        parser = TranscriptParser()
        text = parser.to_plain_text(segments)

        assert "[00:00:01]" in text
        assert "First line" in text
</file>

<file path="tests/test_validator.py">
"""Tests for output validator"""
import pytest
from src.validator import (
    ValidationSeverity,
    ValidationIssue,
    ValidationResult,
    OutputValidator
)
from src.llm_processor import ProcessedChunk


class TestValidationIssue:
    """Test ValidationIssue dataclass"""

    def test_validation_issue_creation(self):
        """Create ValidationIssue with all fields"""
        issue = ValidationIssue(
            severity=ValidationSeverity.ERROR,
            rule="test_rule",
            message="Test error message",
            chunk_index=0,
            snippet="Error context here"
        )
        assert issue.severity == ValidationSeverity.ERROR
        assert issue.rule == "test_rule"
        assert issue.message == "Test error message"
        assert issue.chunk_index == 0
        assert issue.snippet == "Error context here"

    def test_validation_issue_without_optional_fields(self):
        """Create ValidationIssue without optional fields"""
        issue = ValidationIssue(
            severity=ValidationSeverity.WARNING,
            rule="test_warning",
            message="Test warning"
        )
        assert issue.chunk_index is None
        assert issue.snippet is None


class TestValidationResult:
    """Test ValidationResult dataclass"""

    def test_empty_result(self):
        """Create empty ValidationResult"""
        result = ValidationResult()
        assert len(result.issues) == 0
        assert not result.has_errors
        assert not result.has_warnings
        assert result.error_count == 0
        assert result.warning_count == 0

    def test_result_with_errors(self):
        """Result with error issues"""
        result = ValidationResult()
        result.issues.append(ValidationIssue(
            severity=ValidationSeverity.ERROR,
            rule="error_1",
            message="Error 1"
        ))
        result.issues.append(ValidationIssue(
            severity=ValidationSeverity.WARNING,
            rule="warning_1",
            message="Warning 1"
        ))

        assert result.has_errors
        assert result.has_warnings
        assert result.error_count == 1
        assert result.warning_count == 1

    def test_result_to_dict(self):
        """Convert result to dictionary"""
        result = ValidationResult()
        result.issues.append(ValidationIssue(
            severity=ValidationSeverity.ERROR,
            rule="test_error",
            message="Test error",
            chunk_index=0,
            snippet="Test snippet"
        ))

        data = result.to_dict()
        assert data["total_issues"] == 1
        assert data["errors"] == 1
        assert data["warnings"] == 0
        assert len(data["issues"]) == 1
        assert data["issues"][0]["severity"] == "error"
        assert data["issues"][0]["rule"] == "test_error"
        assert data["issues"][0]["chunk"] == 0


class TestOutputValidator:
    """Test OutputValidator class"""

    def test_validate_clean_text_no_issues(self):
        """Validate clean text with no issues"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="This is the original text.",
            cleaned="This is the cleaned text.",
            chunk_index=0
        )
        assert len(issues) == 0

    def test_detect_filler_words(self):
        """Detect common filler words"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Original content",
            cleaned="You know, this is basically like, um, important.",
            chunk_index=0
        )

        # Should detect filler words (warnings)
        filler_issues = [i for i in issues if i.rule == "filler_detected"]
        assert len(filler_issues) > 0
        assert all(i.severity == ValidationSeverity.WARNING for i in filler_issues)

    def test_detect_context_markers(self):
        """Detect context markers as errors"""
        validator = OutputValidator()

        # Test each context marker
        markers = [
            "[CONTEXT FROM PREVIOUS SECTION]",
            "[NEW CONTENT TO PROCESS]",
            "[VIDEO INFO]",
            "[TRANSCRIPT TO PROCESS]"
        ]

        for marker in markers:
            issues = validator.validate_chunk(
                original="Original",
                cleaned=f"Cleaned text {marker} more text",
                chunk_index=0
            )
            context_errors = [i for i in issues if i.rule == "context_marker_in_output"]
            assert len(context_errors) > 0
            assert context_errors[0].severity == ValidationSeverity.ERROR

    def test_valid_timestamp_format(self):
        """Accept valid timestamp formats"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Original",
            cleaned="[00:01:30] This is valid content.\n[00:02:45] More content.",
            chunk_index=0
        )
        timestamp_issues = [i for i in issues if i.rule == "invalid_timestamp_format"]
        assert len(timestamp_issues) == 0

    def test_invalid_timestamp_format(self):
        """Detect invalid timestamp formats"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Original",
            cleaned="[00:01:30.500] Invalid millisecond format.\n[1:30] Missing leading zero.",
            chunk_index=0
        )
        timestamp_issues = [i for i in issues if i.rule == "invalid_timestamp_format"]
        assert len(timestamp_issues) > 0

    def test_excessive_truncation_warning(self):
        """Warn when cleaned text is too short compared to original"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="A" * 1000,  # Long original
            cleaned="Short text",  # Very short cleaned
            chunk_index=0
        )
        truncation_warnings = [i for i in issues if i.rule == "excessive_truncation"]
        assert len(truncation_warnings) > 0

    def test_content_expansion_warning(self):
        """Warn when cleaned text is much longer than original"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Short text",
            cleaned="A" * 1000,  # Much longer cleaned
            chunk_index=0
        )
        expansion_warnings = [i for i in issues if i.rule == "content_expansion"]
        assert len(expansion_warnings) > 0

    def test_detect_many_questions(self):
        """Detect when there are too many questions"""
        validator = OutputValidator()
        questions_text = " ".join([f"Question {i}?" for i in range(5)])
        issues = validator.validate_chunk(
            original="Original",
            cleaned=questions_text,
            chunk_index=0
        )
        question_issues = [i for i in issues if i.rule == "many_questions"]
        assert len(question_issues) > 0
        assert question_issues[0].severity == ValidationSeverity.INFO

    def test_few_questions_no_warning(self):
        """Don't warn for small number of questions"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Original",
            cleaned="Is this good? Yes. What about that? Fine.",
            chunk_index=0
        )
        question_issues = [i for i in issues if i.rule == "many_questions"]
        assert len(question_issues) == 0

    def test_validate_all_chunks(self):
        """Validate multiple processed chunks"""
        validator = OutputValidator()

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original text with filler words like, you know, um.",
                cleaned_text="Cleaned text with problems.",
                input_tokens=50,
                output_tokens=40,
                cost=0.001,
                model="claude-3-5-sonnet-20241022"
            ),
            ProcessedChunk(
                chunk_index=1,
                original_text="Original content that is reasonably long to avoid expansion warning",
                cleaned_text="[CONTEXT FROM PREVIOUS SECTION] Invalid content",
                input_tokens=30,
                output_tokens=25,
                cost=0.0005,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        result = validator.validate_all(chunks)
        assert len(result.issues) > 0
        assert result.has_errors  # Context marker is an error

    def test_validate_clean_transcript(self):
        """Validate transcript that passes all checks"""
        validator = OutputValidator()

        chunk = ProcessedChunk(
            chunk_index=0,
            original_text="[00:00:00] Hello world this is reasonably long original text",
            cleaned_text="[00:00:00] This is clean and properly formatted content.",
            input_tokens=20,
            output_tokens=15,
            cost=0.0003,
            model="claude-3-5-sonnet-20241022"
        )

        result = validator.validate_all([chunk])
        assert not result.has_errors
        assert not result.has_warnings
        assert result.error_count == 0

    def test_fillers_list_patterns(self):
        """Verify filler patterns are regex compatible"""
        validator = OutputValidator()
        import re

        # All filler patterns should be valid regex
        for pattern in validator.FILLERS:
            try:
                re.compile(pattern, re.IGNORECASE)
            except re.error:
                pytest.fail(f"Invalid regex pattern: {pattern}")

    def test_snippet_generation_in_filler_detection(self):
        """Verify snippet generation includes context"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Original",
            cleaned="This is a very long sentence that basically contains the word basically in the middle of it for testing purposes.",
            chunk_index=0
        )

        filler_issues = [i for i in issues if i.rule == "filler_detected"]
        if filler_issues:
            assert filler_issues[0].snippet is not None
            assert "..." in filler_issues[0].snippet

    def test_case_insensitive_filler_detection(self):
        """Detect filler words regardless of case"""
        validator = OutputValidator()
        issues = validator.validate_chunk(
            original="Original",
            cleaned="This is Basically and Actually and REALLY with different cases.",
            chunk_index=0
        )
        filler_issues = [i for i in issues if i.rule == "filler_detected"]
        assert len(filler_issues) > 0
</file>

<file path="tests/test_writer.py">
"""Tests for Markdown writer"""
import pytest
import json
from pathlib import Path
from src.markdown_writer import (
    TranscriptMetadata,
    MarkdownWriter
)
from src.llm_processor import ProcessedChunk
from datetime import datetime


class TestTranscriptMetadata:
    """Test TranscriptMetadata dataclass"""

    def test_metadata_creation(self):
        """Create metadata with all fields"""
        metadata = TranscriptMetadata(
            title="Test Video Title",
            original_duration="00:10:30",
            processed_at="2024-01-15T10:30:00",
            model="claude-3-5-sonnet-20241022",
            total_cost=0.0150,
            chunks_processed=3,
            input_tokens=1500,
            output_tokens=1200
        )
        assert metadata.title == "Test Video Title"
        assert metadata.original_duration == "00:10:30"
        assert metadata.total_cost == 0.0150
        assert metadata.chunks_processed == 3

    def test_metadata_without_duration(self):
        """Create metadata without optional duration"""
        metadata = TranscriptMetadata(
            title="Test",
            original_duration=None,
            processed_at="2024-01-15T10:30:00",
            model="claude-3-5-sonnet-20241022",
            total_cost=0.01,
            chunks_processed=1,
            input_tokens=100,
            output_tokens=80
        )
        assert metadata.original_duration is None


class TestMarkdownWriter:
    """Test MarkdownWriter class"""

    def test_init_creates_output_dir(self, tmp_path):
        """Initialize writer and create output directory"""
        output_dir = tmp_path / "custom_output"
        writer = MarkdownWriter(str(output_dir))
        assert writer.output_dir == output_dir
        assert output_dir.exists()

    def test_init_default_output_dir(self, tmp_path):
        """Initialize with default output directory"""
        writer = MarkdownWriter()
        assert writer.output_dir == Path("output")

    def test_sanitize_filename(self):
        """Sanitize filename by removing invalid characters"""
        writer = MarkdownWriter()
        safe = writer._sanitize_filename('Test/Video:Title?"<>|*File')
        # All invalid chars should be removed
        assert "/" not in safe
        assert ":" not in safe
        assert '"' not in safe
        assert "<" not in safe
        assert ">" not in safe
        assert "|" not in safe
        assert "?" not in safe
        assert "*" not in safe
        # Result should be concatenated without invalid chars
        assert "Test" in safe
        assert "Video" in safe
        assert "Title" in safe
        assert "File" in safe

    def test_sanitize_filename_long_title(self):
        """Truncate long filenames to 50 characters"""
        writer = MarkdownWriter()
        long_title = "A" * 100
        safe = writer._sanitize_filename(long_title)
        assert len(safe) <= 50

    def test_sanitize_filename_replaces_spaces(self):
        """Replace spaces with hyphens"""
        writer = MarkdownWriter()
        safe = writer._sanitize_filename("Test Video Title")
        assert " " not in safe
        assert "-" in safe

    def test_build_markdown(self, tmp_path):
        """Build complete markdown document"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original 1",
                cleaned_text="[00:00:00] First cleaned content.",
                input_tokens=100,
                output_tokens=80,
                cost=0.001,
                model="claude-3-5-sonnet-20241022"
            ),
            ProcessedChunk(
                chunk_index=1,
                original_text="Original 2",
                cleaned_text="[00:01:00] Second cleaned content.",
                input_tokens=120,
                output_tokens=90,
                cost=0.0012,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        metadata = TranscriptMetadata(
            title="Test Video",
            original_duration="00:05:00",
            processed_at="2024-01-15T10:00:00",
            model="claude-3-5-sonnet-20241022",
            total_cost=0.0022,
            chunks_processed=2,
            input_tokens=220,
            output_tokens=170
        )

        md_content = writer._build_markdown(chunks, metadata)

        assert "# Test Video" in md_content
        assert "First cleaned content." in md_content
        assert "Second cleaned content." in md_content
        assert "2024-01-15" in md_content
        assert "claude-3-5-sonnet-20241022" in md_content
        assert "0.0022" in md_content

    def test_metadata_to_dict(self):
        """Convert metadata to dictionary"""
        metadata = TranscriptMetadata(
            title="Test Title",
            original_duration="00:10:00",
            processed_at="2024-01-15T10:00:00",
            model="claude-3-5-sonnet-20241022",
            total_cost=0.015,
            chunks_processed=5,
            input_tokens=1000,
            output_tokens=800
        )

        writer = MarkdownWriter()
        data = writer._metadata_to_dict(metadata)

        assert data["title"] == "Test Title"
        assert data["original_duration"] == "00:10:00"
        assert data["model"] == "claude-3-5-sonnet-20241022"
        assert data["cost_usd"] == 0.015
        assert data["chunks_processed"] == 5
        assert data["tokens"]["input"] == 1000
        assert data["tokens"]["output"] == 800
        assert data["tokens"]["total"] == 1800

    def test_write_creates_files(self, tmp_path):
        """Write markdown and metadata files"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="[00:00:00] Cleaned content.",
                input_tokens=50,
                output_tokens=40,
                cost=0.0005,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        summary = {
            "model": "claude-3-5-sonnet-20241022",
            "total_cost": 0.0005,
            "chunks_processed": 1,
            "total_input_tokens": 50,
            "total_output_tokens": 40
        }

        md_path, json_path = writer.write(
            processed_chunks=chunks,
            title="Test Video",
            summary=summary,
            duration="00:03:00"
        )

        assert md_path.exists()
        assert json_path.exists()
        assert md_path.suffix == ".md"
        assert json_path.name.endswith("-metadata.json")

    def test_write_markdown_content(self, tmp_path):
        """Verify markdown file content"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="[00:00:00] Cleaned transcript content.",
                input_tokens=30,
                output_tokens=25,
                cost=0.0003,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        summary = {
            "model": "claude-3-5-sonnet-20241022",
            "total_cost": 0.0003,
            "chunks_processed": 1,
            "total_input_tokens": 30,
            "total_output_tokens": 25
        }

        md_path, _ = writer.write(
            processed_chunks=chunks,
            title="My Test Video",
            summary=summary
        )

        content = md_path.read_text(encoding="utf-8")
        assert "# My Test Video" in content
        assert "[00:00:00] Cleaned transcript content." in content
        assert "Duration:" not in content  # No duration provided

    def test_write_json_metadata(self, tmp_path):
        """Verify JSON metadata file content"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="Cleaned",
                input_tokens=100,
                output_tokens=80,
                cost=0.001,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        summary = {
            "model": "claude-3-5-sonnet-20241022",
            "total_cost": 0.001,
            "chunks_processed": 1,
            "total_input_tokens": 100,
            "total_output_tokens": 80
        }

        _, json_path = writer.write(
            processed_chunks=chunks,
            title="Test Video",
            summary=summary,
            duration="00:05:00"
        )

        metadata = json.loads(json_path.read_text(encoding="utf-8"))
        assert metadata["title"] == "Test Video"
        assert metadata["original_duration"] == "00:05:00"
        assert metadata["model"] == "claude-3-5-sonnet-20241022"
        assert metadata["cost_usd"] == 0.001
        assert metadata["tokens"]["total"] == 180

    def test_write_multiple_chunks(self, tmp_path):
        """Write multiple chunks in sequence"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="O1",
                cleaned_text="[00:00:00] First chunk.",
                input_tokens=20,
                output_tokens=15,
                cost=0.0002,
                model="claude-3-5-sonnet-20241022"
            ),
            ProcessedChunk(
                chunk_index=1,
                original_text="O2",
                cleaned_text="[00:01:00] Second chunk.",
                input_tokens=25,
                output_tokens=20,
                cost=0.00025,
                model="claude-3-5-sonnet-20241022"
            ),
            ProcessedChunk(
                chunk_index=2,
                original_text="O3",
                cleaned_text="[00:02:00] Third chunk.",
                input_tokens=30,
                output_tokens=25,
                cost=0.0003,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        summary = {
            "model": "claude-3-5-sonnet-20241022",
            "total_cost": 0.00075,
            "chunks_processed": 3,
            "total_input_tokens": 75,
            "total_output_tokens": 60
        }

        md_path, _ = writer.write(
            processed_chunks=chunks,
            title="Multi Chunk Test",
            summary=summary
        )

        content = md_path.read_text(encoding="utf-8")
        assert "First chunk." in content
        assert "Second chunk." in content
        assert "Third chunk." in content
        # Verify order maintained
        first_pos = content.find("First chunk.")
        second_pos = content.find("Second chunk.")
        third_pos = content.find("Third chunk.")
        assert first_pos < second_pos < third_pos

    def test_get_content_for_preview_full(self, tmp_path):
        """Get full preview for short content"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="Short content",
                input_tokens=10,
                output_tokens=8,
                cost=0.0001,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        preview = writer.get_content_for_preview(chunks, max_chars=1000)
        assert "Short content" in preview
        assert "..." not in preview  # Not truncated

    def test_get_content_for_preview_truncated(self, tmp_path):
        """Get truncated preview for long content"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="A" * 10000,  # Very long content
                input_tokens=1000,
                output_tokens=800,
                cost=0.01,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        preview = writer.get_content_for_preview(chunks, max_chars=1000)
        assert len(preview) <= 1100  # Allow some buffer for "..."
        assert "..." in preview  # Should be truncated

    def test_get_content_for_preview_multiple_chunks(self, tmp_path):
        """Preview stops at max_chars across multiple chunks"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="O1",
                cleaned_text="A" * 3000,
                input_tokens=500,
                output_tokens=400,
                cost=0.005,
                model="claude-3-5-sonnet-20241022"
            ),
            ProcessedChunk(
                chunk_index=1,
                original_text="O2",
                cleaned_text="B" * 3000,
                input_tokens=500,
                output_tokens=400,
                cost=0.005,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        preview = writer.get_content_for_preview(chunks, max_chars=5000)
        # Should include first chunk and part of second
        assert "A" in preview
        assert "..." in preview

    def test_get_content_for_preview_empty_chunks(self, tmp_path):
        """Handle empty chunks gracefully"""
        writer = MarkdownWriter(str(tmp_path))
        preview = writer.get_content_for_preview([])
        assert preview == ""

    def test_write_with_special_characters_in_title(self, tmp_path):
        """Write files with special characters in title"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="Cleaned",
                input_tokens=10,
                output_tokens=8,
                cost=0.0001,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        summary = {
            "model": "claude-3-5-sonnet-20241022",
            "total_cost": 0.0001,
            "chunks_processed": 1,
            "total_input_tokens": 10,
            "total_output_tokens": 8
        }

        md_path, json_path = writer.write(
            processed_chunks=chunks,
            title='Video: "What is Python?" (2024)',
            summary=summary
        )

        # Files should be created with sanitized names
        assert md_path.exists()
        assert json_path.exists()
        assert ":" not in md_path.name
        assert '"' not in md_path.name
        assert "?" not in md_path.name

    def test_write_empty_chunks(self, tmp_path):
        """Handle empty cleaned text"""
        writer = MarkdownWriter(str(tmp_path))

        chunks = [
            ProcessedChunk(
                chunk_index=0,
                original_text="Original",
                cleaned_text="   ",  # Whitespace only
                input_tokens=10,
                output_tokens=0,
                cost=0.0,
                model="claude-3-5-sonnet-20241022"
            )
        ]

        summary = {
            "model": "claude-3-5-sonnet-20241022",
            "total_cost": 0.0,
            "chunks_processed": 1,
            "total_input_tokens": 10,
            "total_output_tokens": 0
        }

        md_path, _ = writer.write(
            processed_chunks=chunks,
            title="Empty Test",
            summary=summary
        )

        content = md_path.read_text(encoding="utf-8")
        # Should have header but no content
        assert "# Empty Test" in content
</file>

</files>
